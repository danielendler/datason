name: ğŸ“Š Performance Tracking

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'datason/**'
      - 'benchmarks/**'
      - 'pyproject.toml'
  pull_request:
    branches: [ main ]
    paths:
      - 'datason/**'
      - 'benchmarks/**'
      - 'pyproject.toml'
  schedule:
    # Run weekly to track performance over time
    - cron: '0 8 * * 1'  # Every Monday at 8 AM UTC
  workflow_dispatch:
    inputs:
      save_baseline:
        description: 'Save current results as new baseline'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  pages: write
  id-token: write

env:
  PYTHONUNBUFFERED: 1

jobs:
  performance-test:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better analysis

    - name: ğŸ Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: ğŸ’¾ Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: performance-${{ runner.os }}-py3.11-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          performance-${{ runner.os }}-py3.11-

    - name: ğŸ“¦ Install package and dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest  # Minimal dependencies for performance testing

    - name: ğŸ“‚ Create results directory
      run: mkdir -p benchmarks/results

    - name: ğŸ“¥ Download previous results (if available)
      uses: actions/cache@v4
      with:
        path: benchmarks/results
        key: performance-results-${{ github.repository }}-${{ github.ref_name }}
        restore-keys: |
          performance-results-${{ github.repository }}-main
          performance-results-${{ github.repository }}-

    - name: ğŸš€ Run performance benchmarks
      run: |
        cd benchmarks
        python ci_performance_tracker.py
      env:
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_REF: ${{ github.ref }}
        GITHUB_RUN_ID: ${{ github.run_id }}
        RUNNER_OS: ${{ runner.os }}

    - name: ğŸ“‹ Save baseline (if requested)
      if: ${{ github.event.inputs.save_baseline == 'true' || github.ref == 'refs/heads/main' && github.event_name == 'schedule' }}
      run: |
        cd benchmarks
        cp results/latest.json results/baseline.json
        echo "âœ… Saved new baseline"

    - name: ğŸ“Š Generate performance report
      run: |
        cd benchmarks
        python -c "
        import json
        import os

        # Load latest comparison
        if os.path.exists('results/latest_comparison.json'):
            with open('results/latest_comparison.json', 'r') as f:
                comparison = json.load(f)

            # Create GitHub Actions summary
            with open(os.environ['GITHUB_STEP_SUMMARY'], 'w') as f:
                f.write('# ğŸ“Š Performance Test Results\n\n')

                if comparison['status'] == 'baseline_created':
                    f.write('âœ… **Baseline created successfully**\n\n')
                    f.write('This is the first run, so results have been saved as the new baseline.\n')
                else:
                    f.write(f'ğŸ“‹ **Compared with baseline from:** {comparison[\"baseline_metadata\"].get(\"timestamp\", \"unknown\")}\n\n')

                    if comparison['regressions']:
                        f.write(f'## ğŸ”´ Performance Regressions ({len(comparison[\"regressions\"])})\n\n')
                        f.write('| Test | Change | Current | Baseline |\n')
                        f.write('|------|--------|---------|----------|\n')
                        for reg in comparison['regressions']:
                            f.write(f'| {reg[\"test\"]} | {reg[\"change_pct\"]:+.1f}% | {reg[\"current_ms\"]:.2f}ms | {reg[\"baseline_ms\"]:.2f}ms |\n')
                        f.write('\n')

                    if comparison['improvements']:
                        f.write(f'## ğŸŸ¢ Performance Improvements ({len(comparison[\"improvements\"])})\n\n')
                        f.write('| Test | Change | Current | Baseline |\n')
                        f.write('|------|--------|---------|----------|\n')
                        for imp in comparison['improvements']:
                            f.write(f'| {imp[\"test\"]} | {imp[\"change_pct\"]:+.1f}% | {imp[\"current_ms\"]:.2f}ms | {imp[\"baseline_ms\"]:.2f}ms |\n')
                        f.write('\n')

                    if not comparison['regressions'] and not comparison['improvements']:
                        f.write('ğŸŸ¡ **No significant performance changes detected**\n\n')
                        f.write('All performance metrics are within 5% of baseline values.\n')
        "

    - name: ğŸ’¾ Upload performance artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-results-${{ github.run_id }}
        path: |
          benchmarks/results/*.json
        retention-days: 90

    - name: ğŸ’¾ Save results cache
      uses: actions/cache/save@v4
      if: always()
      with:
        path: benchmarks/results
        key: performance-results-${{ github.repository }}-${{ github.ref_name }}-${{ github.run_id }}

    - name: âŒ Fail if regressions detected
      if: ${{ hashFiles('benchmarks/results/latest_comparison.json') != '' }}
      run: |
        cd benchmarks
        python -c "
        import json
        import sys

        with open('results/latest_comparison.json', 'r') as f:
            comparison = json.load(f)

        if comparison.get('regressions'):
            print(f'âŒ Performance regressions detected: {len(comparison[\"regressions\"])} tests')
            sys.exit(1)
        else:
            print('âœ… No performance regressions detected')
            sys.exit(0)
        "

  # Optional: Generate historical performance charts
  generate-charts:
    runs-on: ubuntu-latest
    needs: performance-test
    if: github.ref == 'refs/heads/main'

    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ğŸ Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: ğŸ“¦ Install chart dependencies
      run: |
        pip install matplotlib seaborn pandas

    - name: ğŸ“¥ Download performance results
      uses: actions/download-artifact@v4
      with:
        name: performance-results-${{ github.run_id }}
        path: benchmarks/results

    - name: ğŸ“Š Generate performance charts
      run: |
        cd benchmarks
        python -c "
        import json
        import matplotlib.pyplot as plt
        import pandas as pd
        from datetime import datetime
        import os
        import glob

        # Collect all historical results
        result_files = glob.glob('results/performance_results_*.json')

        if len(result_files) < 2:
            print('Not enough historical data for charts')
            exit(0)

        data = []
        for file in sorted(result_files):
            with open(file, 'r') as f:
                result = json.load(f)
                timestamp = result['metadata']['timestamp']

                # Extract key metrics
                for category in ['serialization', 'deserialization', 'type_detection']:
                    if category in result['benchmarks']:
                        for test_name, test_data in result['benchmarks'][category].items():
                            if isinstance(test_data, dict) and 'standard' in test_data:
                                data.append({
                                    'timestamp': timestamp,
                                    'test': f'{category}.{test_name}.standard',
                                    'time_ms': test_data['standard']['mean'] * 1000
                                })
                            elif isinstance(test_data, dict) and 'mean' in test_data:
                                data.append({
                                    'timestamp': timestamp,
                                    'test': f'{category}.{test_name}',
                                    'time_ms': test_data['mean'] * 1000
                                })

        if not data:
            print('No performance data found')
            exit(0)

        df = pd.DataFrame(data)
        df['timestamp'] = pd.to_datetime(df['timestamp'])

        # Create chart
        plt.figure(figsize=(12, 8))
        for test in df['test'].unique()[:5]:  # Top 5 tests
            test_data = df[df['test'] == test]
            plt.plot(test_data['timestamp'], test_data['time_ms'], marker='o', label=test)

        plt.title('Datason Performance Over Time')
        plt.xlabel('Date')
        plt.ylabel('Time (ms)')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig('performance_chart.png', dpi=150, bbox_inches='tight')
        print('âœ… Performance chart generated')
        "

    - name: ğŸ“Š Upload performance chart
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-chart-${{ github.run_id }}
        path: benchmarks/performance_chart.png
        retention-days: 30
