# =============================================================================
# Datason v2 — CI Pipeline
#
# Jobs:
#   1. quality   — lint, format, typecheck, architecture, security (fast gate)
#   2. test      — unit + integration across Python 3.10-3.13
#   3. benchmark — performance tracking on PRs (non-blocking)
#   4. build     — wheel + sdist + twine check
# =============================================================================

name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

permissions:
  contents: read

jobs:
  # ---------------------------------------------------------------------------
  # Quality gates — fast, fails early
  # ---------------------------------------------------------------------------
  quality:
    name: Quality Gates
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v5

      - uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true

      - run: uv python install 3.12

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Ruff lint
        run: uv run ruff check datason/ tests/

      - name: Ruff format
        run: uv run ruff format datason/ tests/ --check

      - name: Pyright
        run: uv run pyright --project .

      - name: Architecture — module size (500 lines)
        run: |
          uv run python -c "
          import pathlib, sys
          violations = 0
          for p in sorted(pathlib.Path('datason').rglob('*.py')):
              n = len(p.read_text().splitlines())
              if n > 500:
                  print(f'::error file={p}::{p} has {n} lines (limit: 500)')
                  violations += 1
          if violations:
              sys.exit(1)
          "

      - name: Architecture — function length (50 lines)
        run: |
          uv run python -c "
          import ast, sys, pathlib
          violations = 0
          for path in sorted(pathlib.Path('datason').rglob('*.py')):
              try:
                  tree = ast.parse(path.read_text())
                  for node in ast.walk(tree):
                      if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                          length = (node.end_lineno or 0) - node.lineno + 1
                          if length > 50:
                              print(f'::error file={path},line={node.lineno}::{node.name}() is {length} lines (limit: 50)')
                              violations += 1
              except SyntaxError:
                  pass
          if violations:
              print(f'Found {violations} function(s) exceeding 50 line limit')
              sys.exit(1)
          "

      - name: Security — ruff S rules + pip-audit
        run: |
          uv run ruff check datason/ --select S
          uv run pip-audit

  # ---------------------------------------------------------------------------
  # Tests — matrix across Python versions
  # ---------------------------------------------------------------------------
  test:
    name: Test (Python ${{ matrix.python-version }})
    needs: quality
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.10", "3.13"]
        extras: [""]
        include:
          - python-version: "3.11"
            extras: "--all-extras"
          - python-version: "3.12"
            extras: "--all-extras"
    steps:
      - uses: actions/checkout@v5

      - uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true

      - run: uv python install ${{ matrix.python-version }}

      # Always install dev deps (pytest, etc.); optionally add all extras
      - name: Install dependencies
        run: |
          if [ -n "${{ matrix.extras }}" ]; then
            uv sync --all-extras
          else
            uv sync
          fi

      - name: Run tests
        run: |
          uv run pytest tests/unit/ tests/integration/ \
            --cov=datason \
            --cov-report=xml \
            --cov-report=term-missing \
            -x --tb=short \
            --junitxml=junit-results.xml

      - name: Upload coverage
        if: matrix.python-version == '3.12'
        uses: codecov/codecov-action@v5
        with:
          files: coverage.xml
          flags: unittests
          fail_ci_if_error: false

  # ---------------------------------------------------------------------------
  # Benchmarks — PRs only, non-blocking
  # ---------------------------------------------------------------------------
  benchmark:
    name: Benchmarks
    needs: quality
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    continue-on-error: true
    env:
      BENCHMARK_REGRESSION_THRESHOLD_PCT: "15"
      REALDATA_P95_WARN_THRESHOLD_PCT: "10"
    steps:
      - name: Checkout PR head
        uses: actions/checkout@v5
        with:
          path: head

      - name: Checkout PR base
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.pull_request.base.sha }}
          path: base

      - uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true

      - run: uv python install 3.12

      - name: Install dependencies (base)
        working-directory: base
        run: uv sync --all-extras

      - name: Run base benchmarks
        working-directory: base
        run: |
          uv run pytest -o addopts='' tests/benchmarks/ \
            --benchmark-only \
            --benchmark-sort=mean \
            --benchmark-json=benchmark-base.json

      - name: Run base replay benchmark (real-data sample)
        working-directory: base
        run: |
          uv run python scripts/perf/replay_benchmark.py \
            --input perf/workloads/sample \
            --output perf/results/replay-summary-base.json

      - name: Install dependencies (head)
        working-directory: head
        run: uv sync --all-extras

      - name: Run head benchmarks
        working-directory: head
        run: |
          uv run pytest -o addopts='' tests/benchmarks/ \
            --benchmark-only \
            --benchmark-sort=mean \
            --benchmark-json=benchmark-head.json

      - name: Run head replay benchmark (real-data sample) + compare
        working-directory: head
        run: |
          uv run python scripts/perf/replay_benchmark.py \
            --input perf/workloads/sample \
            --output perf/results/replay-summary-head.json \
            --baseline ../base/perf/results/replay-summary-base.json \
            --p95-threshold-pct $REALDATA_P95_WARN_THRESHOLD_PCT \
            --report-markdown ../benchmark-realdata-comparison.md \
            --fail-on-regression \
            --profile-top-regressions 3

      - name: Compare benchmark results (base vs head)
        run: |
          python3 - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          base_path = Path("base/benchmark-base.json")
          head_path = Path("head/benchmark-head.json")
          threshold = float(os.environ.get("BENCHMARK_REGRESSION_THRESHOLD_PCT", "15"))

          if not base_path.exists() or not head_path.exists():
              print("Missing benchmark JSON files for comparison.")
              sys.exit(1)

          base_data = json.loads(base_path.read_text())
          head_data = json.loads(head_path.read_text())

          def to_index(payload: dict) -> dict[str, float]:
              index: dict[str, float] = {}
              for bench in payload.get("benchmarks", []):
                  name = bench.get("fullname") or bench.get("name")
                  stats = bench.get("stats", {})
                  mean = stats.get("mean")
                  if name and isinstance(mean, (int, float)):
                      index[name] = float(mean)
              return index

          base_idx = to_index(base_data)
          head_idx = to_index(head_data)
          common = sorted(set(base_idx) & set(head_idx))

          if not common:
              print("No overlapping benchmarks found between base and head.")
              sys.exit(1)

          regressions: list[tuple[str, float, float, float]] = []
          improvements: list[tuple[str, float, float, float]] = []
          stable = 0

          for name in common:
              b = base_idx[name]
              h = head_idx[name]
              if b <= 0:
                  continue
              delta_pct = ((h - b) / b) * 100.0
              if delta_pct > threshold:
                  regressions.append((name, b, h, delta_pct))
              elif delta_pct < -threshold:
                  improvements.append((name, b, h, delta_pct))
              else:
                  stable += 1

          summary = []
          summary.append("## Benchmark Comparison (PR head vs base)")
          summary.append("")
          summary.append(f"- Regression threshold: `{threshold:.1f}%`")
          summary.append(f"- Compared benchmarks: `{len(common)}`")
          summary.append(f"- Regressions: `{len(regressions)}`")
          summary.append(f"- Improvements: `{len(improvements)}`")
          summary.append(f"- Stable: `{stable}`")
          summary.append("")

          if regressions:
              summary.append("### Regressions")
              summary.append("| Benchmark | Base mean (s) | Head mean (s) | Delta |")
              summary.append("|---|---:|---:|---:|")
              for name, b, h, d in sorted(regressions, key=lambda x: x[3], reverse=True)[:20]:
                  summary.append(f"| `{name}` | {b:.6f} | {h:.6f} | +{d:.2f}% |")
              summary.append("")

          if improvements:
              summary.append("### Improvements")
              summary.append("| Benchmark | Base mean (s) | Head mean (s) | Delta |")
              summary.append("|---|---:|---:|---:|")
              for name, b, h, d in sorted(improvements, key=lambda x: x[3])[:20]:
                  summary.append(f"| `{name}` | {b:.6f} | {h:.6f} | {d:.2f}% |")
              summary.append("")

          report = "\n".join(summary) + "\n"
          Path("benchmark-comparison.md").write_text(report)

          step_summary = os.environ.get("GITHUB_STEP_SUMMARY")
          if step_summary:
              with open(step_summary, "a", encoding="utf-8") as f:
                  f.write(report)

          if regressions:
              print(f"Detected {len(regressions)} benchmark regressions over {threshold:.1f}%")
              sys.exit(1)

          print("No benchmark regressions beyond threshold.")
          PY

      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: |
            base/benchmark-base.json
            head/benchmark-head.json
            benchmark-comparison.md
            base/perf/results/replay-summary-base.json
            head/perf/results/replay-summary-head.json
            benchmark-realdata-comparison.md
            head/perf/results/profiles/
          if-no-files-found: ignore

  # ---------------------------------------------------------------------------
  # Build — validate wheel + sdist
  # ---------------------------------------------------------------------------
  build:
    name: Build Package
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v5

      - uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true

      - run: uv python install 3.12

      - run: uv sync

      - name: Build wheel + sdist
        run: uv build

      - name: Check package metadata
        run: uvx twine check dist/*

      - uses: actions/upload-artifact@v4
        with:
          name: dist
          path: dist/
