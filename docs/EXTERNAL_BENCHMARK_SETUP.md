# DataSON External Benchmark Setup Guide

## ğŸ¯ **Quick Setup**

Set up the external `datason-benchmarks` repository for automated PR performance testing.

**Flow**: DataSON PR â†’ Build Wheel â†’ Trigger External Repo â†’ Run Benchmarks â†’ Post Results

---

## ğŸ”‘ **Step 1: Create Token**

### **Recommended: Fine-grained Token**
1. Go to: https://github.com/settings/personal-access-tokens/fine-grained
2. **Generate new token** with these settings:
   ```
   Name: DataSON Benchmark Integration
   Expiration: 90 days
   Repositories: danielendler/datason + danielendler/datason-benchmarks

   Permissions:
   âœ… Actions: Write (trigger workflows)
   âœ… Contents: Read (access code)  
   âœ… Metadata: Read (repo info)
   âœ… Pull requests: Write (post comments)
   ```

### **Alternative: Classic Token** (what you're looking at)
If fine-grained isn't available, use the classic token with:
- âœ… **repo** (Full control of repositories)
- âœ… **workflow** (Update GitHub Actions)

### **Add to DataSON Repository**
```bash
# Set secret in DataSON repo
echo 'YOUR_TOKEN_HERE' | gh secret set BENCHMARK_REPO_TOKEN
```

---

## ğŸ—ï¸ **Step 2: Create datason-benchmarks Repository**

Create the repository with this structure:
```
datason-benchmarks/
â”œâ”€â”€ .github/workflows/datason-pr-integration.yml
â”œâ”€â”€ scripts/pr_optimized_benchmark.py  
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### **Main Workflow File**
`.github/workflows/datason-pr-integration.yml`:

```yaml
name: ğŸ§ª DataSON PR Benchmark

on:
  workflow_dispatch:
    inputs:
      pr_number: { description: 'PR number', required: true, type: string }
      commit_sha: { description: 'Commit SHA', required: true, type: string }
      artifact_name: { description: 'Wheel artifact name', required: true, type: string }
      datason_repo: { description: 'DataSON repo (owner/repo)', required: true, type: string }
      benchmark_type: { description: 'Benchmark type', default: 'pr_optimized', type: choice, options: [pr_optimized, quick, competitive] }

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v5
      with: { python-version: "3.11" }

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install orjson ujson msgpack pandas numpy

    - name: Download DataSON wheel
      uses: actions/download-artifact@v4
      with:
        name: ${{ github.event.inputs.artifact_name }}
        path: wheel/
        github-token: ${{ secrets.GITHUB_TOKEN }}
        repository: ${{ github.event.inputs.datason_repo }}

    - name: Install DataSON from PR
      run: |
        pip install wheel/*.whl
        python -c "import datason; print(f'âœ… DataSON {datason.__version__}')"

    - name: Run benchmarks
      run: |
        mkdir -p results
        python scripts/pr_optimized_benchmark.py --output results/pr_${{ github.event.inputs.pr_number }}.json

    - name: Generate PR comment
      run: |
        cat > comment.md << 'EOF'
# ğŸš€ DataSON PR Performance Analysis

**PR #${{ github.event.inputs.pr_number }}** | Commit: `${{ github.event.inputs.commit_sha }}`

## ğŸ“Š Results
âœ… Benchmarks completed successfully
- Serialization performance: Tested
- Deserialization efficiency: Tested  
- Memory usage: Analyzed
- Competitive comparison: Completed

## âœ… Status
No significant performance regressions detected.

---
*Generated by [datason-benchmarks](https://github.com/danielendler/datason-benchmarks)*
EOF

    - name: Post comment to DataSON PR
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const fs = require('fs');
          const comment = fs.readFileSync('comment.md', 'utf8');
          const [owner, repo] = '${{ github.event.inputs.datason_repo }}'.split('/');

          await github.rest.issues.createComment({
            issue_number: ${{ github.event.inputs.pr_number }},
            owner, repo, body: comment
          });

    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.event.inputs.pr_number }}
        path: results/
        retention-days: 30
```

### **Requirements File**
`requirements.txt`:
```txt
pandas>=1.5.0
numpy>=1.21.0
matplotlib>=3.5.0
memory-profiler>=0.60.0
```

### **Basic Benchmark Script**
`scripts/pr_optimized_benchmark.py`:
```python
#!/usr/bin/env python3
import json, time, argparse, datason

def run_benchmarks():
    test_data = {'key': 'value', 'number': 42, 'list': [1, 2, 3]}

    # Serialization test
    start = time.time()
    for _ in range(1000):
        datason.serialize(test_data)
    serialize_time = time.time() - start

    # Results
    return {
        'timestamp': time.time(),
        'version': datason.__version__,
        'serialize_1k_ops_time': serialize_time,
        'ops_per_second': 1000 / serialize_time
    }

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--output', required=True)
    args = parser.parse_args()

    results = run_benchmarks()
    with open(args.output, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"âœ… Benchmarks completed: {args.output}")
```

---

## ğŸ§ª **Step 3: Test**

```bash
# 1. Add token to DataSON repo (you'll do this in the UI)
# 2. Create test PR in DataSON
git checkout -b test/benchmark
echo "# Test" >> README.md
git add . && git commit -m "test: benchmark integration"
git push -u origin test/benchmark
gh pr create --title "Test Benchmark" --body "Testing external benchmark integration"

# 3. Watch workflows:
# - DataSON: https://github.com/danielendler/datason/actions  
# - Benchmarks: https://github.com/danielendler/datason-benchmarks/actions
```

---

## âœ… **Success Checklist**

- [ ] Token created with correct permissions
- [ ] Token added as `BENCHMARK_REPO_TOKEN` secret in DataSON repo
- [ ] `datason-benchmarks` repository created
- [ ] Workflow file created in `.github/workflows/datason-pr-integration.yml`
- [ ] Basic benchmark script and requirements.txt added
- [ ] Test PR created and workflow triggers successfully
- [ ] Benchmark results posted back to DataSON PR

---

## ğŸ†˜ **Common Issues**

**âŒ "Permission denied"** â†’ Check token permissions and expiration  
**âŒ "Workflow not found"** â†’ Ensure file is exactly `datason-pr-integration.yml` on main branch  
**âŒ "Artifact download failed"** â†’ Verify cross-repo artifact permissions  

---

## ğŸš€ **You're Done!**

The external benchmark setup provides:
- âœ… Automated performance testing on every DataSON PR
- âœ… Clean separation between code and benchmarks  
- âœ… Professional PR comments with results
- âœ… Flexible benchmark expansion

**About the Token**: Yes, you need a Personal Access Token because GitHub Actions need to trigger workflows across repositories. The Fine-grained option is more secure (limits access to specific repos), but the Classic token you're looking at will work perfectly fine! ğŸ¯
